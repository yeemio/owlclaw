# 实现计划: e2e-validation

## 文档联动

- requirements: `.kiro/specs/e2e-validation/requirements.md`
- design: `.kiro/specs/e2e-validation/design.md`
- tasks: `.kiro/specs/e2e-validation/tasks.md`
- status source: `.kiro/specs/SPEC_TASKS_SCAN.md`

## 概述

本实现计划将端到端验证系统分解为一系列增量开发任务。系统使用 Python 实现，采用模块化架构，支持 mionyee 任务验证、决策质量对比、组件集成测试、性能基准测试、错误注入测试和并发测试。

实现将按照以下顺序进行：
1. 核心数据模型和接口定义
2. 测试场景管理器
3. 执行引擎和数据收集器
4. 对比引擎和报告生成器
5. 测试编排器
6. 集成和端到端测试

## 任务

- [x] 1. 设置项目结构和核心接口
  - 创建 Python 项目结构（`owlclaw/e2e/`、`tests/e2e/`、`config/`）
  - 配置 Python 质量工具（ruff、mypy）
  - 安装依赖：hypothesis（属性测试）、pytest（单元/集成测试）
  - 定义核心类型和接口（TestScenario、ExecutionResult、ValidationConfig 等）
  - _需求: 1.1, 2.1, 3.1_

- [x] 2. 实现测试场景管理器
  - [x] 2.1 实现 TestScenarioManager 类
    - 实现 createScenario、getScenario、updateScenario、deleteScenario 方法
    - 实现场景配置验证逻辑
    - 实现场景存储（使用 JSON 文件或内存存储）
    - _需求: 4.1, 4.2_
  
  - [x]* 2.2 编写 TestScenarioManager 的属性测试
    - **属性 11: 测试场景 CRUD 往返**
    - **验证需求: 4.1**
  
  - [x]* 2.3 编写场景配置验证的属性测试
    - **属性 12: 场景配置验证**
    - **验证需求: 4.2**
  
  - [x] 2.4 实现场景导入导出功能
    - 实现 exportScenarios 和 importScenarios 方法
    - 支持 JSON 格式的序列化和反序列化
    - _需求: 4.5_
  
  - [x]* 2.5 编写导入导出的属性测试
    - **属性 15: 场景导入导出往返**
    - **验证需求: 4.5**

- [x] 3. 实现数据收集器
  - [x] 3.1 实现 DataCollector 类
    - 实现 startCollection、recordEvent、recordMetric、recordError、stopCollection 方法
    - 实现事件和指标的内存缓冲
    - 实现执行轨迹的记录逻辑
    - _需求: 1.4, 2.2, 6.1, 6.2_
  
  - [x]* 3.2 编写数据收集的单元测试
    - 测试事件记录的正确性
    - 测试指标收集的完整性
    - _需求: 1.4, 6.1_

- [x] 4. 实现执行引擎
  - [x] 4.1 实现 ExecutionEngine 类基础功能
    - 实现 executeScenario 方法框架
    - 实现与被测系统的集成接口（Cron Trigger、Agent Runtime 等）
    - 实现执行轨迹记录
    - _需求: 1.1, 1.2, 1.3, 3.1_
  
  - [x] 4.2 实现 Mionyee 任务执行逻辑
    - 实现 executeMionyeeTask 方法
    - 集成 Cron Trigger 触发逻辑
    - 集成 Agent Runtime 调用
    - 集成 Skills System、Governance Layer、Hatchet Integration
    - _需求: 1.1, 1.2, 1.3_
  
  - [x]* 4.3 编写 Mionyee 任务执行的属性测试
    - **属性 1: Mionyee 任务完整执行流程**
    - **验证需求: 1.1, 1.2, 1.3**
  
  - [x]* 4.4 编写执行轨迹完整性的属性测试
    - **属性 2: 执行轨迹完整性**
    - **验证需求: 1.4**
  
  - [x] 4.5 实现错误注入功能
    - 实现 injectError 方法
    - 支持不同类型的错误注入（超时、网络故障、资源不足）
    - 实现错误传播和恢复机制
    - _需求: 7.1, 7.2, 7.5_
  
  - [x]* 4.6 编写错误注入的属性测试
    - **属性 24: 错误注入能力**
    - **属性 25: 错误处理验证**
    - **验证需求: 7.1, 7.2, 7.5_
  
  - [x] 4.7 实现测试环境清理
    - 实现 cleanup 方法
    - 自动清理测试数据和临时资源
    - _需求: 4.4_
  
  - [x]* 4.8 编写测试数据清理的属性测试
    - **属性 14: 测试数据清理**
    - **验证需求: 4.4**

- [x] 5. 检查点 - 确保所有测试通过
  - 确保所有测试通过，如有问题请询问用户。

- [x] 6. 实现对比引擎
  - [x] 6.1 实现 ComparisonEngine 类
    - 实现 compare 方法，对比 V3 Agent 和 Original Cron 的执行结果
    - 实现 calculateDecisionQuality 方法，计算决策质量指标
    - 实现 comparePerformance 方法，对比性能指标
    - 实现 detectAnomalies 方法，检测异常差异
    - _需求: 2.1, 2.2, 2.3, 2.5_
  
  - [x]* 6.2 编写双系统执行的属性测试
    - **属性 4: 双系统执行**
    - **验证需求: 2.1**
  
  - [x]* 6.3 编写决策指标完整性的属性测试
    - **属性 5: 决策指标完整性**
    - **验证需求: 2.2**
  
  - [x]* 6.4 编写差异计算正确性的属性测试
    - **属性 6: 差异计算正确性**
    - **验证需求: 2.3**
  
  - [x]* 6.5 编写异常检测的属性测试
    - **属性 8: 异常检测**
    - **验证需求: 2.5**

- [x] 7. 实现报告生成器
  - [x] 7.1 实现 ReportGenerator 类
    - 实现 generateValidationReport 方法
    - 实现 generateComparisonReport 方法
    - 实现 generatePerformanceReport 方法
    - 实现报告数据结构和格式化逻辑
    - _需求: 1.5, 2.4, 5.1, 5.2_
  
  - [x] 7.2 实现报告可视化功能
    - 集成图表库（如 Chart.js 或 D3.js）
    - 实现图表生成逻辑（折线图、柱状图、饼图等）
    - _需求: 2.4, 5.3_
  
  - [x] 7.3 实现报告导出功能
    - 实现 exportReport 方法
    - 支持 JSON、HTML、PDF 格式导出
    - _需求: 5.4_
  
  - [x]* 7.4 编写验证报告生成的属性测试
    - **属性 3: 验证报告生成**
    - **属性 16: 综合报告完整性**
    - **验证需求: 1.5, 5.1, 5.2**
  
  - [x]* 7.5 编写对比报告的属性测试
    - **属性 7: 对比报告完整性**
    - **属性 17: 对比报告可视化**
    - **验证需求: 2.4, 5.3**
  
  - [x]* 7.6 编写报告导出的属性测试
    - **属性 18: 报告格式导出**
    - **验证需求: 5.4**
  
  - [x]* 7.7 编写失败报告的属性测试
    - **属性 19: 失败报告详细信息**
    - **验证需求: 5.5**

- [x] 8. 实现性能基准测试功能
  - [x] 8.1 实现性能指标收集
    - 扩展 DataCollector 以支持性能指标收集
    - 实现响应时间、吞吐量、资源使用的测量
    - _需求: 6.1, 6.2_
  
  - [x] 8.2 实现性能基准管理
    - 实现基准阈值的设置和更新
    - 实现性能指标与基准的对比
    - 实现性能警告触发逻辑
    - _需求: 6.3, 6.4_
  
  - [x] 8.3 实现性能趋势分析
    - 实现历史性能数据的存储
    - 实现趋势分析算法
    - _需求: 6.5_
  
  - [x]* 8.4 编写性能指标收集的属性测试
    - **属性 20: 性能指标收集完整性**
    - **验证需求: 6.1, 6.2**
  
  - [x]* 8.5 编写性能警告的属性测试
    - **属性 21: 性能警告触发**
    - **验证需求: 6.3**
  
  - [x]* 8.6 编写基准阈值管理的属性测试
    - **属性 22: 基准阈值管理**
    - **验证需求: 6.4**

- [x] 9. 实现并发测试功能
  - [x] 9.1 实现并发执行引擎
    - 扩展 ExecutionEngine 以支持并发任务执行
    - 实现线程池和任务队列
    - 实现资源锁定和队列机制
    - _需求: 8.1, 8.3, 8.4_
  
  - [x]* 9.2 编写并发执行正确性的属性测试
    - **属性 27: 并发执行正确性**
    - **验证需求: 8.1, 8.5**
  
  - [x]* 9.3 编写并发性能的属性测试
    - **属性 28: 并发性能保证**
    - **验证需求: 8.2**
  
  - [x]* 9.4 编写资源竞争处理的属性测试
    - **属性 29: 资源竞争处理**
    - **验证需求: 8.3**

- [x] 10. 实现测试编排器
  - [x] 10.1 实现 TestOrchestrator 类
    - 实现 runFullValidation 方法，协调完整的验证流程
    - 实现 runMionyeeTask 方法
    - 实现 runDecisionComparison 方法
    - 实现 runIntegrationTests 方法
    - 集成所有已实现的组件
    - _需求: 1.1, 2.1, 3.1_
  
  - [x] 10.2 实现测试生命周期管理
    - 实现测试的启动、执行、停止逻辑
    - 实现测试超时处理
    - 实现测试结果聚合
    - _需求: 1.5, 5.1_
  
  - [x]* 10.3 编写组件集成链的属性测试
    - **属性 9: 组件集成链完整性**
    - **验证需求: 3.1, 3.2, 3.3, 3.4**
  
  - [x]* 10.4 编写错误传播的属性测试
    - **属性 10: 错误传播和回滚**
    - **验证需求: 3.5**

- [x] 11. 实现测试数据隔离
  - [x] 11.1 实现测试环境隔离机制
    - 实现测试数据库和测试配置的隔离
    - 实现测试数据的自动清理
    - _需求: 4.3, 4.4_
  
  - [x]* 11.2 编写测试数据隔离的属性测试
    - **属性 13: 测试数据隔离**
    - **验证需求: 4.3**

- [x] 12. 检查点 - 确保所有测试通过
  - 确保所有测试通过，如有问题请询问用户。

- [x] 13. 实现 CLI 和配置
  - [x] 13.1 实现命令行接口
    - 使用 argparse（Python）实现 CLI
    - 支持运行不同类型的测试（mionyee、对比、集成、性能、并发）
    - 支持配置文件加载
    - _需求: 1.1, 2.1, 3.1_
  
  - [x] 13.2 实现配置管理
    - 实现配置文件的加载和验证
    - 支持环境变量覆盖
    - 提供默认配置
    - _需求: 所有需求_

- [x] 14. 集成测试和端到端测试
  - [x]* 14.1 编写 Mionyee 任务 1 的端到端测试
    - 测试完整的任务 1 执行流程
    - 验证所有组件的集成
    - _需求: 1.1_
  
  - [x]* 14.2 编写 Mionyee 任务 2 的端到端测试
    - 测试完整的任务 2 执行流程
    - _需求: 1.2_
  
  - [x]* 14.3 编写 Mionyee 任务 3 的端到端测试
    - 测试完整的任务 3 执行流程
    - _需求: 1.3_
  
  - [x]* 14.4 编写决策对比的端到端测试
    - 测试 V3 Agent vs Original Cron 的完整对比流程
    - _需求: 2.1, 2.2, 2.3_
  
  - [x]* 14.5 编写错误注入场景的端到端测试
    - 测试各种错误注入场景
    - 验证错误处理和恢复
    - _需求: 7.1, 7.2, 7.3_
  
  - [x]* 14.6 编写并发场景的端到端测试
    - 测试高并发场景
    - 验证数据一致性
    - _需求: 8.1, 8.2, 8.3_

- [x] 15. 文档和示例
  - [x] 15.1 编写 README 文档
    - 项目概述和架构说明
    - 安装和配置指南
    - 使用示例
  
  - [x] 15.2 编写 API 文档
    - 使用 Markdown 维护 API 文档
    - 为主要接口添加详细注释
  
  - [x] 15.3 创建示例测试场景
    - 创建 Mionyee 任务的示例场景
    - 创建决策对比的示例场景
    - 创建错误注入的示例场景

- [ ] 16. 实现历史回放测试引擎（需求 9）
  - [x] 16.1 实现 EventImporter
    - 支持 CSV/JSON 格式导入历史事件序列
    - 事件验证（时间戳格式、必填字段、payload 结构）
    - 按时间戳排序 + 时间范围过滤
  - [x] 16.2 实现 ReplayScheduler
    - 加速模式：忽略事件间时间间隔，顺序执行
    - 实时模式：模拟真实时间间隔（asyncio.sleep）
  - [x] 16.3 实现 ReplayEngine
    - 串联 EventImporter → ReplayScheduler → Agent/Cron 执行
    - 分别在 V3 Agent 和 Original Cron 上回放
    - 记录每个事件的 Agent 决策（capability 选择、参数、结果）
  - [ ] 16.4 实现 ReplayComparator
    - 计算决策一致率（Agent vs 历史实际决策）
    - 计算决策偏差分布（按 low/medium/high/critical 分类）
    - 计算时间序列决策质量趋势
    - 记录 Agent 记忆增长曲线（评估学习效果）
  - [ ] 16.5 实现 ReplayReport
    - 生成回放对比报告（含可视化图表）
    - 高亮关键偏差事件
    - _需求: 9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7_

- [ ] 17. 实现 Shadow Mode（需求 10）
  - [ ] 17.1 实现 ShadowModeInterceptor
    - 拦截 Agent 的 capability 调用（记录但不执行）
    - 返回模拟成功结果
    - 与 Agent Runtime 集成（在 EXECUTING 阶段拦截）
  - [ ] 17.2 实现 ShadowComparator
    - 实时对比 Agent 决策与 Cron 实际执行结果
    - 存储对比记录到数据库
    - 计算实时一致率
  - [ ] 17.3 实现 ShadowDashboard 数据接口
    - 一致率实时指标
    - 决策差异详情列表
    - 按天的质量趋势
    - LLM 累计成本
    - 自动切换建议（连续 7 天一致率 > 90%）
  - [ ] 17.4 实现 migration_weight 协同
    - migration_weight=0.0 时纯 Shadow Mode
    - migration_weight 变化时 Shadow/Agent 比例自动调整
    - 质量下降自动回退 migration_weight
    - _需求: 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7_

- [ ] 18. 实现 A/B 测试（需求 11）
  - [ ] 18.1 实现 ABTestRunner
    - 基于 migration_weight 的随机分组
    - Agent 组和 fallback 组的指标记录
  - [ ] 18.2 实现统计显著性检验
    - t-test / chi-square 检验
    - 自动判断 Agent 组是否优于 fallback 组
  - [ ] 18.3 实现自动 migration_weight 调整
    - 基于 A/B 结果自动提升或回退 weight
    - Agent 组显著劣于 fallback 组时自动告警 + 回退
    - _需求: 11.1, 11.2, 11.3, 11.4, 11.5_

- [ ] 19. 最终检查点 - 确保所有测试通过
  - 运行完整的测试套件
  - 验证所有功能正常工作
  - 确保代码质量和测试覆盖率达标
  - 如有问题请询问用户。

## 注意事项

- 标记为 `*` 的任务是测试重点标记，不代表可跳过；发布前需完成并通过验收
- 每个任务都引用了具体的需求，以确保可追溯性
- 检查点任务确保增量验证
- 属性测试验证通用正确性属性
- 单元测试验证特定示例和边缘情况
- 使用 hypothesis 进行属性测试，每个测试至少运行 100 次迭代
