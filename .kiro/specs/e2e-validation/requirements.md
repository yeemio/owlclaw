# 需求文档

## 文档联动

- requirements: `.kiro/specs/e2e-validation/requirements.md`
- design: `.kiro/specs/e2e-validation/design.md`
- tasks: `.kiro/specs/e2e-validation/tasks.md`
- status source: `.kiro/specs/SPEC_TASKS_SCAN.md`

> **目标**：验证 OwlClaw 从触发器到运行时与治理层的端到端行为正确性，并量化 Agent 决策质量  
> **优先级**：P1  
> **预估工作量**：8-12 天

## 简介

本文档定义了 OwlClaw Agent 系统的端到端验证需求。该系统需要验证从 Cron 触发器到 Agent 运行时、治理层、Skills 系统以及 Hatchet 集成的完整工作流程。验证包括 mionyee 的三个核心任务的端到端测试，以及 v3 Agent 与原始 cron 实现之间的决策质量对比测试。

## 术语表

- **E2E_Validator**: 端到端验证系统
- **Agent_Runtime**: Agent 运行时系统
- **Cron_Trigger**: 基于时间的任务触发器
- **Governance_Layer**: 治理层，负责策略和权限管理
- **Skills_System**: Skills 能力系统
- **Hatchet_Integration**: Hatchet 工作流集成
- **Mionyee_Task**: mionyee 用户的三个核心任务
- **Decision_Quality**: 决策质量指标
- **V3_Agent**: 第三版 Agent 实现
- **Original_Cron**: 原始 cron 实现
- **Test_Scenario**: 测试场景
- **Validation_Report**: 验证报告

## 需求

### 需求 1: Mionyee 任务端到端验证

**用户故事:** 作为系统测试人员，我希望验证 mionyee 的三个核心任务能够端到端正确执行，以确保系统集成的正确性。

#### 验收标准

1. 当触发 mionyee 任务 1 时，系统应当通过 Cron_Trigger 启动、经过 Agent_Runtime 处理、调用 Skills_System、通过 Governance_Layer 验证、并在 Hatchet_Integration 中完成执行
2. 当触发 mionyee 任务 2 时，系统应当通过 Cron_Trigger 启动、经过 Agent_Runtime 处理、调用 Skills_System、通过 Governance_Layer 验证、并在 Hatchet_Integration 中完成执行
3. 当触发 mionyee 任务 3 时，系统应当通过 Cron_Trigger 启动、经过 Agent_Runtime 处理、调用 Skills_System、通过 Governance_Layer 验证、并在 Hatchet_Integration 中完成执行
4. 当任何 mionyee 任务执行时，系统应当记录完整的执行轨迹，包括每个组件的输入和输出
5. 当任何 mionyee 任务完成时，系统应当生成包含执行时间、状态和结果的验证报告

### 需求 2: 决策质量对比测试

**用户故事:** 作为系统架构师，我希望对比 V3_Agent 和 Original_Cron 的决策质量，以评估新系统的改进效果。

#### 验收标准

1. 当执行相同的测试场景时，系统应当分别使用 V3_Agent 和 Original_Cron 执行并记录决策结果
2. 当收集决策数据时，系统应当记录决策准确性、响应时间、资源使用和错误率等指标
3. 当对比决策质量时，系统应当计算 V3_Agent 和 Original_Cron 之间的差异百分比
4. 当生成对比报告时，系统应当包含可视化图表和统计分析结果
5. 当决策质量差异超过阈值时，系统应当标记异常并提供详细分析

### 需求 3: 组件集成验证

**用户故事:** 作为开发人员，我希望验证各个组件之间的集成正确性，以确保系统的可靠性。

#### 验收标准

1. 当 Cron_Trigger 触发任务时，系统应当正确传递任务上下文到 Agent_Runtime
2. 当 Agent_Runtime 处理任务时，系统应当正确调用 Skills_System 并获取响应
3. 当 Skills_System 执行时，系统应当通过 Governance_Layer 验证权限和策略
4. 当任务需要工作流编排时，系统应当正确使用 Hatchet_Integration 进行任务调度
5. 当任何组件失败时，系统应当正确传播错误并触发回滚机制

### 需求 4: 测试数据管理

**用户故事:** 作为测试工程师，我希望管理测试数据和场景，以支持可重复的验证测试。

#### 验收标准

1. 系统应当提供创建、读取、更新和删除测试场景的功能
2. 当创建测试场景时，系统应当验证场景配置的完整性和有效性
3. 当执行测试时，系统应当使用隔离的测试数据，不影响生产环境
4. 当测试完成时，系统应当自动清理测试数据和临时资源
5. 系统应当支持导入和导出测试场景配置

### 需求 5: 验证报告生成

**用户故事:** 作为项目经理，我希望获得详细的验证报告，以了解系统的质量状态。

#### 验收标准

1. 当验证测试完成时，系统应当生成包含所有测试结果的综合报告
2. 当生成报告时，系统应当包含测试覆盖率、成功率、失败原因和性能指标
3. 当报告包含对比数据时，系统应当提供清晰的可视化图表
4. 系统应当支持以 JSON、HTML 和 PDF 格式导出报告
5. 当测试失败时，系统应当在报告中包含详细的错误堆栈和调试信息

### 需求 6: 性能基准测试

**用户故事:** 作为性能工程师，我希望建立性能基准，以监控系统性能变化。

#### 验收标准

1. 当执行性能测试时，系统应当测量端到端响应时间、吞吐量和资源使用
2. 当收集性能数据时，系统应当记录每个组件的性能指标
3. 当性能指标超出基准范围时，系统应当发出警告
4. 系统应当支持设置和更新性能基准阈值
5. 当生成性能报告时，系统应当包含历史趋势分析

### 需求 7: 错误注入测试

**用户故事:** 作为可靠性工程师，我希望测试系统的容错能力，以确保系统在异常情况下的稳定性。

#### 验收标准

1. 系统应当支持在任何组件中注入模拟错误
2. 当注入错误时，系统应当验证错误处理和恢复机制是否正确工作
3. 当组件失败时，系统应当验证其他组件是否能够正确处理失败情况
4. 当执行错误注入测试时，系统应当记录错误传播路径和恢复时间
5. 系统应当支持配置不同类型的错误场景（超时、网络故障、资源不足等）

### 需求 8: 并发测试

**用户故事:** 作为系统测试人员，我希望验证系统在并发场景下的正确性，以确保系统的可扩展性。

#### 验收标准

1. 当多个任务并发执行时，系统应当正确处理所有任务而不产生竞态条件
2. 当并发负载增加时，系统应当保持响应时间在可接受范围内
3. 当资源竞争发生时，系统应当正确实施资源锁定和队列机制
4. 系统应当支持配置并发测试的线程数和任务数
5. 当并发测试完成时，系统应当验证数据一致性和完整性

### 需求 9: 历史回放测试（Historical Replay）

**用户故事:** 作为系统架构师，我希望用真实的历史业务数据回放过去的事件序列，以量化 Agent 决策与原始 cron 决策的差异。

#### 验收标准

1. 系统应当支持导入历史事件序列数据（CSV/JSON 格式，包含时间戳、事件类型、payload）
2. 当执行历史回放时，系统应当按时间戳顺序依次触发 Agent Run，模拟真实事件流
3. 系统应当记录 Agent 在每个历史事件上的决策（capability 选择、参数、结果）
4. 系统应当支持同一历史数据分别在 V3 Agent 和 Original Cron 上回放，并生成对比报告
5. 当回放完成时，系统应当计算以下指标：
   - 决策一致率（Agent 决策 vs 历史最优决策）
   - 决策偏差分布（按严重程度分类）
   - 时间序列中的决策质量变化趋势（Agent 是否随记忆积累而改善）
6. 系统应当支持指定回放的时间范围（如最近 30 天）
7. 系统应当支持加速回放（忽略事件间的等待时间）和实时回放（模拟真实时间间隔）

### 需求 10: Shadow Mode（影子模式）

**用户故事:** 作为系统管理员，我希望在生产环境中让 Agent 和 cron 同时运行，Agent 的决策只记录不执行，以便安全地验证 Agent 决策质量。

#### 验收标准

1. 系统应当支持 Shadow Mode 运行模式，其中 Agent Run 正常执行但 capability 调用被拦截为只读
2. 当 Shadow Mode 启用时，Agent 的所有 function calling 决策应当完整记录（包括选择的 capability、参数、决策理由）
3. 当 Shadow Mode 启用时，Original Cron 继续正常执行业务逻辑
4. 系统应当实时对比 Agent 决策与 cron 实际执行结果，记录差异
5. 系统应当提供 Shadow Mode 仪表板，展示：
   - Agent vs Cron 决策一致率（实时）
   - 决策差异详情（每次不一致的详细对比）
   - Agent 决策质量趋势（随时间变化）
   - LLM 成本累计
6. 当 Shadow Mode 运行满足预设条件时（如连续 7 天一致率 > 90%），系统应当建议切换到 Agent 模式
7. Shadow Mode 应当与 migration_weight 机制协同：
   - migration_weight=0.0 时 = 纯 Shadow Mode（Agent 不执行）
   - migration_weight=0.1~0.9 时 = 部分 Shadow + 部分 Agent 执行
   - migration_weight=1.0 时 = 完全 Agent 执行

### 需求 11: A/B 测试

**用户故事:** 作为系统架构师，我希望在 Shadow Mode 验证通过后，让 Agent 逐步接管部分任务并量化实际效果。

#### 验收标准

1. 系统应当支持通过 migration_weight 控制 Agent 接管比例
2. 当 A/B 测试运行时，系统应当记录 Agent 执行组和 fallback 执行组的业务指标
3. 系统应当支持统计显著性检验（如 chi-square 或 t-test），判断 Agent 组是否优于 fallback 组
4. 系统应当支持自动调整 migration_weight（基于 A/B 结果自动提升或回退）
5. 当 A/B 测试发现 Agent 组显著劣于 fallback 组时，系统应当自动回退 migration_weight 并发出告警


## 功能需求（FR）

- **FR-1：Mionyee 三任务端到端验证**（对应需求 1）
- **FR-2：V3 Agent vs Original Cron 决策质量对比**（对应需求 2）
- **FR-3：核心组件集成验证**（对应需求 3）
- **FR-4：测试场景与数据管理**（对应需求 4）
- **FR-5：验证报告生成与导出**（对应需求 5）
- **FR-6：性能基准测试与阈值告警**（对应需求 6）
- **FR-7：错误注入与恢复验证**（对应需求 7）
- **FR-8：并发一致性与可扩展性验证**（对应需求 8）
- **FR-9：历史回放测试**（对应需求 9）
- **FR-10：Shadow Mode 验证**（对应需求 10）
- **FR-11：A/B 测试与 migration_weight 自动调节**（对应需求 11）

## 非功能需求（NFR）

- **NFR-1：技术栈一致性**  
  E2E 验证工具链使用 Python（pytest + hypothesis），不引入核心 TS 运行时依赖。
- **NFR-2：可重复性**  
  同一输入场景在固定随机种子下应可重现验证结果。
- **NFR-3：可追踪性**  
  每次验证任务必须生成可回溯执行轨迹与报告工件。
- **NFR-4：隔离性**  
  测试数据与生产数据严格隔离，不污染业务环境。

## 验收标准总览

### 功能验收
- [ ] **FR-1**：三个核心任务均可完整跑通并生成轨迹
- [ ] **FR-2**：Agent 与 Cron 对比指标可计算且可视化
- [ ] **FR-3**：触发器/运行时/治理层集成断言全部通过
- [ ] **FR-4**：场景 CRUD、导入导出、清理机制可用
- [ ] **FR-5**：报告支持 JSON/HTML/PDF 导出
- [ ] **FR-6**：性能基线可设置并触发阈值告警
- [ ] **FR-7**：错误注入可执行且恢复路径可验证
- [ ] **FR-8**：并发场景下一致性断言通过
- [ ] **FR-9**：历史回放可按时间序列复现并统计指标
- [ ] **FR-10**：Shadow Mode 支持只记录不执行并实时对比
- [ ] **FR-11**：A/B 显著性分析与权重自动调节可执行

### 非功能验收
- [ ] **NFR-1**：验证框架不依赖 TypeScript 运行时
- [ ] **NFR-2**：固定种子下关键指标可重现
- [ ] **NFR-3**：每轮验证生成完整工件（日志/轨迹/报告）
- [ ] **NFR-4**：测试环境清理后无残留业务数据
